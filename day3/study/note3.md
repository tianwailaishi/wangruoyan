# DAY3 Note
## 深度学习

### 1.激活函数
在深度学习中，激活函数（Activation Function）负责引入非线性，使神经网络能够学习复杂的映射关系。以下是几种常见的激活函数：

#### 1️⃣ Sigmoid（S型激活函数）
**特点：**
- 输出范围 \((0,1)\)，适用于概率建模。

**缺点：**
- **梯度消失问题**：当 \(x\) 绝对值较大时，梯度趋近于 0，影响深层网络的学习。
- **计算相对较慢**：包含指数运算。

✅ **适用场景**： 早期神经网络、二分类任务（用于输出层）。

---

#### 2️⃣ Tanh（双曲正切函数）
**特点：**
- 输出范围 \((-1,1)\)，中心对称，比 Sigmoid 更适合零均值数据。

**缺点：**
- **仍然存在梯度消失问题**。

✅ **适用场景**： 适用于 RNN 等需要中心对称输出的任务，比 Sigmoid 更优。

---

#### 3️⃣ ReLU（Rectified Linear Unit，修正线性单元）
**特点：**
- 计算简单，收敛快，大大缓解梯度消失问题。

**缺点：**
- **死亡 ReLU 问题**：当 \(x < 0\) 时，梯度为 0，神经元可能永远不会被激活。

✅ **适用场景**： CNN、深度网络的默认激活函数，计算高效。

---

#### 4️⃣ Leaky ReLU（带泄漏的 ReLU）
**特点：**
- 解决死亡 ReLU 问题，引入一个小的斜率 \(\alpha\)（如 0.01）。
- 保持计算效率，避免神经元失活。

✅ **适用场景**： 深度 CNN、回归任务。

---

#### 5️⃣ Parametric ReLU（PReLU，参数化 ReLU）
**特点：**
- 动态调整负半轴斜率，比 Leaky ReLU 更灵活。
- 适用于大型数据集，增强网络表达能力。

✅ **适用场景**： 计算机视觉任务，如 ImageNet 训练。

---

#### 6️⃣ ELU（Exponential Linear Unit）
**特点：**
- 负半轴更平滑，避免了 Leaky ReLU 可能的不稳定行为。
- 计算比 ReLU 稍慢，但梯度表现更好。

✅ **适用场景**： 深度学习任务，特别是在较深网络中能稳定收敛。

---

#### 7️⃣ Swish（Google 提出的自门控激活函数）
**特点：**
- 自适应平滑激活，兼顾 ReLU 和 Sigmoid 优点。

**缺点：**
- **计算比 ReLU 复杂**。

✅ **适用场景**： Google EfficientNet 等大规模网络。

---

#### 8️⃣ Softmax（用于分类任务的激活函数）
**特点：**
- 多分类任务专用，可以将输出转换为概率分布。
- 适用于神经网络的最后一层，通常与 **交叉熵损失（Cross-Entropy Loss）** 一起使用。

✅ **适用场景**： 适用于多分类任务，如 NLP、图像分类等。

---

#### ✅ 总结对比

| 激活函数 | 输出范围 | 计算复杂度 | 梯度消失 | 额外参数 | 适用场景 |
|----------|---------|-----------|----------|----------|----------|
| **Sigmoid** | (0,1) | 高 | 有 | 无 | 早期神经网络、二分类任务 |
| **Tanh** | (-1,1) | 高 | 有 | 无 | RNN、零均值数据 |
| **ReLU** | [0,∞) | 低 | 有 | 无 | CNN、深度网络 |
| **Leaky ReLU** | (-∞,∞) | 低 | 无 | 有 | 深度网络，防止神经元死亡 |
| **PReLU** | (-∞,∞) | 低 | 无 | 有 | 计算机视觉，灵活学习负半轴参数 |
| **ELU** | (-∞,∞) | 中等 | 无 | 有 | 深度网络，稳定收敛 |
| **Swish** | (-∞,∞) | 高 | 无 | 有 | Google 高效网络 |
| **Softmax** | (0,1) 且归一化 | 高 | 无 | 无 | 多分类任务 |

### 2.训练自己的数据集
- a.数据集预处理，新建工程项目
- b.先对数据集进行分类，分为train（70%）和val（30%）
- c.对分类好的数据集打标签，生成train.txt和val.txt文件
- d.加载数据集